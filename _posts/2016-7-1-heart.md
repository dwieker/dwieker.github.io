---
layout: post
title: Touchless Heart Rate Monitor
---

<iframe width="560" height="315" src="https://www.youtube.com/embed/ciPy2Ac-Hbc#t=30s" frameborder="0" allowfullscreen></iframe>

A few years back, MIT released a paper describing a ridiculously cool method of measuring a person's heart rate with just a camera. Every time your heart beats, blood vessels near the surface of the skin on your face dilate and slightly alter the absorbtion of incident light. The effect is too feignt to see with human eyes, but can be picked up on with a decent camera and signal processing technqiues. The above video is a demo of some hopefully easy to use [open source python software I wrote to do just that.](https://github.com/dwieker/FaceTrack). You'll need openCV installed to use it.

Here's a general overview of the methods used:   
1. Accurately track the user's face frame-to-frame using a Haar Cascade Classifier  
2. Take a spatial average of the RGB values near the center of the user's face every frame   
3. Apply independent component analysis to the RBG time series.   
4. Take the FFT of the components and filter for signals within a reasonable range for a heart rate.  
5. Pick the most promenent frequency.  

Here's some more detailed explanation, if you're interested!

1. Face Tracking  
[A Haar Cascade Classifier](http://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html) is a computationally fast feauture-based method of object detection, commonly used in computer vision. It slides a small window over an image and calcualtes a "cascade" of feautres inside the window with increasing complexity. To search for a face, for example, it might first check whether the upper region of the window is darker than the lower (which is common in faces because of the eyes), then check for things like bilateral symetery, edge-features and center features (like the the nose). If enough relevant features are observed in the window, it detects the object. Haar Classifiers are trained with a large number of images to teach the algorithm what specific features to look for. I used the OpenCV implementation in this project. 

2. RGB Measurement  
With the face located, isolate a region of pixels in the center of the face(near the nose/eyes/lower forehead) and take the mean of Red, Blue and Green values over the region. If you do this everyframe frame, and the lighting is good enough, you can get a signal that looks like this:

![Output](https://github.com/dwieker/dwieker.github.io/blob/master/images/series.png?raw=true)



