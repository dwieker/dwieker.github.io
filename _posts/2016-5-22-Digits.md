---
layout: post
title: Classifying Handwritten Digits
---
![](https://raw.githubusercontent.com/dwieker/dwieker.github.io/master/images/Screenshot%20from%202016-05-22%2009%3A21%3A30.png)

For our third project at metis, my group and I tried our hand at handwritten digit classification. More specifically, given a scan or image of a page full of digits, locate and classify each inidivdual digit on the page. 

The classic [MNIST data set](http://yann.lecun.com/exdb/mnist/) provided a set of 60k images to train and test our machine learning algorithms on. Initially, I tried breaking each image into a set of about 10 features, including simple statistical properties like mean horizontal/vertical position of pixels, symmetry, total size of the digit, radial moment of intertia, and average number of edges when treversing from left to right and down to up. We then applied a host of classifiction algorithms on this feature set -- Logistic Regression, Support Vector Machine and Naive bayes, but ultimately Random Forrest generated the best result, with 91% of the MNIST test data accurately identified by the model.  

However, 91% isn't enough to be very USEFUL. Given an image of digits, the resulting text scan would be full of typos and hardly worthwhile. So we shifted to a new method: [Convolutional Neural Networks.](https://en.wikipedia.org/wiki/Convolutional_neural_network) "Convolutional" refers to the initial convolution step of the process, in which the image is convolved with a set of kernals, each with a small receptive field. The convolutional layer is great for image identification because it is translationally invariant and focuses on locally connected features. In other words, the position of the digit in the image is irrelevant, and pixels on opposite side of the image do not strongly influence each other in the learning process, as opposed to a more traditional fully connected neural network. 

Our convolutional neural network led to a 99.2% accuracy on the MNIST data set!

The next challenge was segmenting indiviudal digits in an image before passing them off into our neural network. First, we binarized the image. This wasn't as easy as it seems; shading variation over the image made it impossible to choose a global threshold to binarize the image on (pixel values below thresh -> 0, pixel values above -> 1) To get around this, we apply the "sobel" operator. It essentially finds the gradient/derivative/rate of change of the image. Consequently, regions that quickly transition from one intenisty to another pop out -- It's an edge detector! This caused the shading variation to drop out and the digits to clearly pop out on the page. Binarizing this new image was then feasible.


![](https://raw.githubusercontent.com/dwieker/dwieker.github.io/master/images/Screenshot%20from%202016-05-22%2009%3A20%3A01.png)

After applying the gradient, we're left with a bunch of outlined digits. The neural network was trained on a set of solid digits, so simply passing these outlines into the network would not result in very accurate identification. To get around this, we applied a few morphological operations: mainly "closing" and "filling." Below is an example of that process.


![](https://raw.githubusercontent.com/dwieker/dwieker.github.io/master/images/Screenshot%20from%202016-05-22%2009%3A21%3A04.png)

With this final image, we can label each isolated blob of connected pixels, segement them, and pass them of to our classifacation model!


